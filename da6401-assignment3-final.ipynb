{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11876572,"sourceType":"datasetVersion","datasetId":7464076},{"sourceId":11876589,"sourceType":"datasetVersion","datasetId":7464083}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n## Installation\n","metadata":{"id":"ltJHb5ag7zli"}},{"cell_type":"code","source":"pip install wandb numpy pandas matplotlib torch torchvision keras","metadata":{"id":"plCZD7Bu7kSi","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:21.183233Z","iopub.execute_input":"2025-05-22T01:52:21.183508Z","iopub.status.idle":"2025-05-22T01:52:24.372295Z","shell.execute_reply.started":"2025-05-22T01:52:21.183475Z","shell.execute_reply":"2025-05-22T01:52:24.371403Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Libraries","metadata":{"id":"HPBByXJv74V3"}},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport wandb\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"id":"ByjM-xIR73sg","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:24.373843Z","iopub.execute_input":"2025-05-22T01:52:24.374097Z","iopub.status.idle":"2025-05-22T01:52:30.090910Z","shell.execute_reply.started":"2025-05-22T01:52:24.374071Z","shell.execute_reply":"2025-05-22T01:52:30.090069Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Dataset Loader","metadata":{"id":"YneL_yxf8K5C"}},{"cell_type":"code","source":"def load_data(train_path, val_path, test_path):\n  paths = [train_path, val_path, test_path]\n  datasets = []\n  for path in paths:\n    dataset = []\n    with open(path, encoding=\"utf-8\") as file:\n      lines = file.readlines()\n      for line in lines:\n        cols = line.strip().split(\"\\t\")\n        if(len(cols) != 3):\n          continue\n        dataset.append((cols[0].strip(), cols[1].strip()))\n    datasets.append(dataset)\n  return datasets","metadata":{"id":"wK9JPn-n8KSW","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:30.092167Z","iopub.execute_input":"2025-05-22T01:52:30.092644Z","iopub.status.idle":"2025-05-22T01:52:30.097913Z","shell.execute_reply.started":"2025-05-22T01:52:30.092617Z","shell.execute_reply":"2025-05-22T01:52:30.096921Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Build Vocabulary","metadata":{"id":"6bkA5fnHW9yv"}},{"cell_type":"code","source":"def buildVocabulary(data):\n  input_characters = set()\n  target_characters = set()\n  for input, target in data:\n    for char in input:\n      input_characters.add(char)\n    for char in target:\n      target_characters.add(char)\n\n  input_characters = [' '] + sorted(list(input_characters))\n  target_characters = [' ', '\\t', '\\n']+ sorted(list(target_characters))\n  return input_characters, target_characters","metadata":{"id":"yt9IdSitXcLR","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:30.099100Z","iopub.execute_input":"2025-05-22T01:52:30.099380Z","iopub.status.idle":"2025-05-22T01:52:30.116803Z","shell.execute_reply.started":"2025-05-22T01:52:30.099355Z","shell.execute_reply":"2025-05-22T01:52:30.116301Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def generateTokenIndices(input_characters, target_characters):\n  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n  return input_token_index, target_token_index","metadata":{"id":"lHTNFmiM0xzT","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:31.748191Z","iopub.execute_input":"2025-05-22T01:52:31.748438Z","iopub.status.idle":"2025-05-22T01:52:31.752446Z","shell.execute_reply.started":"2025-05-22T01:52:31.748421Z","shell.execute_reply":"2025-05-22T01:52:31.751724Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# def generateEmbeddings(data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length):\n#   encoder_input_data = np.zeros(\n#       (len(data), max_encoder_seq_length, len(input_token_index)), dtype=\"float32\")\n#   decoder_input_data = np.zeros(\n#       (len(data), max_decoder_seq_length, len(target_token_index)), dtype=\"float32\")\n#   decoder_target_data = np.zeros(\n#       (len(data), max_decoder_seq_length, len(target_token_index)), dtype=\"float32\")\n#   for i, (input, target) in enumerate(data):\n#     target = \"\\t\" + target + \"\\n\"\n#     for t, char in enumerate(input):\n#       encoder_input_data[i, t, input_token_index[char]] = 1.0\n#     if t + 1 < max_encoder_seq_length:\n#       encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n#     for t, char in enumerate(target):\n#       decoder_input_data[i, t, target_token_index[char]] = 1.0\n#       if t > 0:\n#         decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n#     decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n#     decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n#   return encoder_input_data, decoder_input_data, decoder_target_data\n\ndef generateEmbeddings(data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length):\n    encoder_input_data = np.zeros((len(data), max_encoder_seq_length), dtype=\"int64\")\n    decoder_input_data = np.zeros((len(data), max_decoder_seq_length), dtype=\"int64\")\n    decoder_target_data = np.zeros((len(data), max_decoder_seq_length), dtype=\"int64\")\n\n    for i, (input_text, target_text) in enumerate(data):\n        target_text = \"\\t\" + target_text + \"\\n\"  # Add start and end tokens\n\n        # Fill encoder sequence\n        for t, char in enumerate(input_text):\n            encoder_input_data[i, t] = input_token_index.get(char, input_token_index[\" \"])\n        for t in range(len(input_text), max_encoder_seq_length):\n            encoder_input_data[i, t] = input_token_index[\" \"]\n\n        # Fill decoder input and target\n        for t, char in enumerate(target_text):\n            decoder_input_data[i, t] = target_token_index.get(char, target_token_index[\" \"])\n            if t > 0:\n                decoder_target_data[i, t - 1] = target_token_index.get(char, target_token_index[\" \"])\n        for t in range(len(target_text), max_decoder_seq_length):\n            decoder_input_data[i, t] = target_token_index[\" \"]\n            if t > 0:\n                decoder_target_data[i, t - 1] = target_token_index[\" \"]\n\n    #print(\"train_enc shape:\", encoder_input_data.shape)\n    #print(\"train_enc dtype:\", encoder_input_data.dtype)\n    return encoder_input_data, decoder_input_data, decoder_target_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:33.350917Z","iopub.execute_input":"2025-05-22T01:52:33.351205Z","iopub.status.idle":"2025-05-22T01:52:33.358432Z","shell.execute_reply.started":"2025-05-22T01:52:33.351183Z","shell.execute_reply":"2025-05-22T01:52:33.357580Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# train_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.train.tsv\"\n# val_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.dev.tsv\"\n# test_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.test.tsv\"\n# train_data, val_data, test_data = load_data(train_path, val_path, test_path)\n# input_characters, target_characters = buildVocabulary(train_data)\n# input_token_index, target_token_index = generateTokenIndices(input_characters, target_characters)\n# max_encoder_seq_length = max([len(input) for input, _ in train_data])\n# max_decoder_seq_length = max([len(target) for _, target in train_data]) + 2\n\n# train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = generateEmbeddings(train_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n# val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = generateEmbeddings(val_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n# test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = generateEmbeddings(test_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)","metadata":{"id":"2fCqCXk83rQm","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T15:50:53.599124Z","iopub.execute_input":"2025-05-21T15:50:53.599783Z","iopub.status.idle":"2025-05-21T15:50:54.930699Z","shell.execute_reply.started":"2025-05-21T15:50:53.599757Z","shell.execute_reply":"2025-05-21T15:50:54.929909Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # ==================== MODEL ====================\n# class Seq2SeqModel(nn.Module):\n#     def __init__(self, config, input_vocab_size, target_vocab_size):\n#         super().__init__()\n    #     self.config = config\n\n    #     self.embedding_encoder = nn.Embedding(input_vocab_size, config.embedding_dim, padding_idx=0)\n    #     self.embedding_decoder = nn.Embedding(target_vocab_size, config.embedding_dim, padding_idx=0)\n\n    #     rnn_model_type = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[config.cell_type]\n\n    #     self.encoder = rnn_model_type(\n    #         input_size=config.embedding_dim,\n    #         hidden_size=config.hidden_size,\n    #         num_layers=config.encoder_layers,\n    #         batch_first=True,\n    #         dropout=config.dropout if config.encoder_layers > 1 else 0,\n    #         bidirectional=False\n    #     )\n\n    #     self.decoder = rnn_model_type(\n    #         input_size=config.embedding_dim,\n    #         hidden_size=config.hidden_size,\n    #         num_layers=config.decoder_layers,\n    #         batch_first=True,\n    #         dropout=config.dropout if config.decoder_layers > 1 else 0,\n    #         bidirectional=False\n    #     )\n\n    #     self.fc_out = nn.Linear(config.hidden_size, target_vocab_size)\n\n    # def forward(self, encoder_input, decoder_input):\n    #     encoder_embedded = self.embedding_encoder(encoder_input)\n    #     decoder_embedded = self.embedding_decoder(decoder_input)\n\n    #     print(encoder_embedded.shape)\n    #     print(decoder_embedded.shape)\n\n    #     if self.config.cell_type == 'LSTM':\n    #         encoder_outputs, (h, c) = self.encoder(encoder_embedded)\n    #         decoder_outputs, _ = self.decoder(decoder_embedded, (h, c))\n    #     else:\n    #         encoder_outputs, h = self.encoder(encoder_embedded)\n    #         decoder_outputs, _ = self.decoder(decoder_embedded, h)\n\n    #     output = self.fc_out(decoder_outputs)\n    #     return output\n\nimport torch.nn as nn\nimport torch\n\nclass Seq2SeqModel(nn.Module):\n    def __init__(self, config, input_vocab_size, target_vocab_size):\n        super().__init__()\n        self.config = config\n\n        self.embedding_encoder = nn.Embedding(input_vocab_size, config.embedding_dim, padding_idx=0)\n        self.embedding_decoder = nn.Embedding(target_vocab_size, config.embedding_dim, padding_idx=0)\n\n        rnn_model_type = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[config.cell_type]\n\n        self.encoder = rnn_model_type(\n            input_size=config.embedding_dim,\n            hidden_size=config.hidden_size,\n            num_layers=config.encoder_layers,\n            batch_first=True,\n            dropout=config.dropout if config.encoder_layers > 1 else 0,\n            bidirectional=False\n        )\n\n        self.decoder = rnn_model_type(\n            input_size=config.embedding_dim,\n            hidden_size=config.hidden_size,\n            num_layers=config.decoder_layers,\n            batch_first=True,\n            dropout=config.dropout if config.decoder_layers > 1 else 0,\n            bidirectional=False\n        )\n\n        # Add projection layers to map encoder hidden state to decoder hidden state if needed\n        self.h_proj = nn.Linear(config.hidden_size * config.encoder_layers,\n                                config.hidden_size * config.decoder_layers)\n\n        if config.cell_type == 'LSTM':\n            self.c_proj = nn.Linear(config.hidden_size * config.encoder_layers,\n                                    config.hidden_size * config.decoder_layers)\n        else:\n            self.c_proj = None\n\n        self.fc_out = nn.Linear(config.hidden_size, target_vocab_size)\n\n    def _transform_hidden(self, h_enc):\n        # h_enc shape: (layers, batch, hidden_size)\n        batch_size = h_enc.size(1)\n        h_flat = h_enc.permute(1, 0, 2).contiguous().view(batch_size, -1)  # (batch, layers*hidden_size)\n        h_proj = self.h_proj(h_flat)  # (batch, dec_layers*hidden_size)\n        h_proj = h_proj.view(batch_size, self.config.decoder_layers, self.config.hidden_size).permute(1, 0, 2).contiguous()\n        return h_proj\n\n    def _transform_cell(self, c_enc):\n        if c_enc is None:\n            return None\n        batch_size = c_enc.size(1)\n        c_flat = c_enc.permute(1, 0, 2).contiguous().view(batch_size, -1)\n        c_proj = self.c_proj(c_flat)\n        c_proj = c_proj.view(batch_size, self.config.decoder_layers, self.config.hidden_size).permute(1, 0, 2).contiguous()\n        return c_proj\n\n    def forward(self, encoder_input, decoder_input):\n        encoder_embedded = self.embedding_encoder(encoder_input)\n        decoder_embedded = self.embedding_decoder(decoder_input)\n\n        # print shapes for debugging\n        #print(encoder_embedded.shape)\n        #print(decoder_embedded.shape)\n\n        if self.config.cell_type == 'LSTM':\n            encoder_outputs, (h, c) = self.encoder(encoder_embedded)\n            h_dec = self._transform_hidden(h)\n            c_dec = self._transform_cell(c)\n            decoder_outputs, _ = self.decoder(decoder_embedded, (h_dec, c_dec))\n        else:\n            encoder_outputs, h = self.encoder(encoder_embedded)\n            h_dec = self._transform_hidden(h)\n            decoder_outputs, _ = self.decoder(decoder_embedded, h_dec)\n\n        output = self.fc_out(decoder_outputs)\n        return output\n\n\n\n# ==================== TRAINING ====================\ndef train_model(model, train_data, val_data, config, target_vocab_size, device):\n    model.to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)   # ignoring padding values in loss (padded with zero here)\n    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n\n    train_enc, train_dec, train_tgt = train_data\n    val_enc, val_dec, val_tgt = val_data\n\n    train_enc = torch.tensor(train_enc, dtype=torch.long)\n    train_dec = torch.tensor(train_dec, dtype=torch.long)\n    train_tgt = torch.tensor(train_tgt, dtype=torch.long)\n\n    #print(f\"After torch tensor conversion shape: f{train_enc.shape} , f{train_dec.shape}, f{train_tgt.shape}\")\n\n    val_enc = torch.tensor(val_enc, dtype=torch.long)\n    val_dec = torch.tensor(val_dec, dtype=torch.long)\n    val_tgt = torch.tensor(val_tgt, dtype=torch.long)\n\n    dataset = TensorDataset(train_enc, train_dec, train_tgt)\n    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n    start_token_idx = 1  # '\\t' in target_token_index\n    end_token_idx = 2    # '\\n' in target_token_index\n\n    for epoch in range(config.epochs):\n        model.train()\n        total_loss = 0\n\n        for encoder_batch, decoder_input_batch, decoder_target_batch in train_loader:\n            optimizer.zero_grad()\n            output = model(encoder_batch.to(device), decoder_input_batch.to(device))\n            loss = criterion(output.view(-1, target_vocab_size), decoder_target_batch.to(device).view(-1))\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            \n        wandb.log({\n            \"epoch\" : epoch+1,\n            \"train_loss\": total_loss,\n            \n        })\n\n        model.eval()\n        with torch.no_grad():\n            correct_tokens = 0\n            total_tokens = 0\n            val_loss = 0\n\n            val_dataset = TensorDataset(val_enc, val_dec, val_tgt)\n            val_loader = DataLoader(val_dataset, batch_size=32)\n\n            for val_enc_batch, val_dec_batch, val_tgt_batch in val_loader:\n                val_enc_batch = val_enc_batch.to(device)\n                val_dec_batch = val_dec_batch.to(device)\n                val_tgt_batch = val_tgt_batch.to(device) \n\n                output = model(val_enc_batch, val_dec_batch)\n                loss = criterion(output.view(-1, target_vocab_size), val_tgt_batch.view(-1))\n                val_loss += loss.item()                \n\n                predictions = output.argmax(dim=-1)\n                mask = val_tgt_batch != 0  # ignore padding\n                correct_tokens += (predictions == val_tgt_batch).masked_select(mask).sum().item()\n                total_tokens += mask.sum().item()\n\n            val_accuracy = correct_tokens / total_tokens\n\n            wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n        #print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:37.172989Z","iopub.execute_input":"2025-05-22T01:52:37.173528Z","iopub.status.idle":"2025-05-22T01:52:37.190669Z","shell.execute_reply.started":"2025-05-22T01:52:37.173502Z","shell.execute_reply":"2025-05-22T01:52:37.189877Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ==================== SWEEP CONFIG ====================\nsweep_config = {\n    'name': 'sweep_final1',\n    'method': 'bayes',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'embedding_dim': {'values': [16, 32, 64]},\n        'hidden_size': {'values': [32, 64, 128]},\n        'encoder_layers': {'values': [1, 2]},\n        'decoder_layers': {'values': [1, 2]},\n        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n        'dropout': {'values': [0.2, 0.3]},\n        'lr': {'values': [0.001]},\n        'epochs': {'value': 10},\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:40.123386Z","iopub.execute_input":"2025-05-22T01:52:40.124014Z","iopub.status.idle":"2025-05-22T01:52:40.129779Z","shell.execute_reply.started":"2025-05-22T01:52:40.123983Z","shell.execute_reply":"2025-05-22T01:52:40.128992Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def sweep_train():\n    wandb.init()\n    config = wandb.config\n\n    train_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.train.tsv\"\n    val_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.dev.tsv\"\n    test_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.test.tsv\"\n    train_data, val_data, test_data = load_data(train_path, val_path, test_path)\n    input_characters, target_characters = buildVocabulary(train_data)\n    input_token_index, target_token_index = generateTokenIndices(input_characters, target_characters)\n    max_encoder_seq_length = max([len(input) for input, _ in train_data])\n    max_decoder_seq_length = max([len(target) for _, target in train_data]) + 2\n    \n    train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = generateEmbeddings(train_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n    val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = generateEmbeddings(val_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n    test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = generateEmbeddings(test_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n\n\n    model = Seq2SeqModel(config, len(input_token_index), len(target_token_index))\n    train_model(model, (train_encoder_input_data, train_decoder_input_data, train_decoder_target_data), (val_encoder_input_data, val_decoder_input_data, val_decoder_target_data), config, len(target_token_index), device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:40.758291Z","iopub.execute_input":"2025-05-22T01:52:40.758793Z","iopub.status.idle":"2025-05-22T01:52:40.764473Z","shell.execute_reply.started":"2025-05-22T01:52:40.758770Z","shell.execute_reply":"2025-05-22T01:52:40.763615Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"wandb.login(key=\"af9e3b2810916ad653fca8fdddab4c68df36b740\")\nsweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment3',entity=\"nikhithaa-iit-madras\")\nwandb.agent(sweep_id, function=sweep_train, count=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T01:52:43.424752Z","iopub.execute_input":"2025-05-22T01:52:43.425031Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikhithaa\u001b[0m (\u001b[33mnikhithaa-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: i6pogcjt\nSweep URL: https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yb5efjel with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015256-yb5efjel</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel' target=\"_blank\">neat-sweep-1</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>1231.54235</td></tr><tr><td>val_accuracy</td><td>0.81161</td></tr><tr><td>val_loss</td><td>102.90616</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">neat-sweep-1</strong> at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel</a><br> View project at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250522_015256-yb5efjel/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4mkqoq24 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015443-4mkqoq24</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24' target=\"_blank\">genial-sweep-2</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>563.46399</td></tr><tr><td>val_accuracy</td><td>0.8779</td></tr><tr><td>val_loss</td><td>62.30132</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">genial-sweep-2</strong> at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24</a><br> View project at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250522_015443-4mkqoq24/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9wxlw2ka with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015619-9wxlw2ka</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka' target=\"_blank\">lively-sweep-3</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇█▇▇█▇</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>3503.59712</td></tr><tr><td>val_accuracy</td><td>0.38551</td></tr><tr><td>val_loss</td><td>348.67116</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lively-sweep-3</strong> at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka</a><br> View project at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250522_015619-9wxlw2ka/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3i1wsj9a with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015740-3i1wsj9a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/3i1wsj9a' target=\"_blank\">devoted-sweep-4</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/3i1wsj9a' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/3i1wsj9a</a>"},"metadata":{}}],"execution_count":null}]}