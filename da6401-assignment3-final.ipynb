{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltJHb5ag7zli"
   },
   "source": [
    "\n",
    "\n",
    "## Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:21.183508Z",
     "iopub.status.busy": "2025-05-22T01:52:21.183233Z",
     "iopub.status.idle": "2025-05-22T01:52:24.372295Z",
     "shell.execute_reply": "2025-05-22T01:52:24.371403Z",
     "shell.execute_reply.started": "2025-05-22T01:52:21.183475Z"
    },
    "id": "plCZD7Bu7kSi",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb numpy pandas matplotlib torch torchvision keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPBByXJv74V3"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:24.374097Z",
     "iopub.status.busy": "2025-05-22T01:52:24.373843Z",
     "iopub.status.idle": "2025-05-22T01:52:30.090910Z",
     "shell.execute_reply": "2025-05-22T01:52:30.090069Z",
     "shell.execute_reply.started": "2025-05-22T01:52:24.374071Z"
    },
    "id": "ByjM-xIR73sg",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import wandb\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YneL_yxf8K5C"
   },
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:30.092644Z",
     "iopub.status.busy": "2025-05-22T01:52:30.092167Z",
     "iopub.status.idle": "2025-05-22T01:52:30.097913Z",
     "shell.execute_reply": "2025-05-22T01:52:30.096921Z",
     "shell.execute_reply.started": "2025-05-22T01:52:30.092617Z"
    },
    "id": "wK9JPn-n8KSW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# preprocessing all train,test,validation dataset files\n",
    "def load_data(train_path, val_path, test_path):\n",
    "  paths = [train_path, val_path, test_path]\n",
    "  datasets = []\n",
    "  for path in paths:\n",
    "    dataset = []\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "      lines = file.readlines()\n",
    "      for line in lines:\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        if(len(cols) != 3):   # skipping malformed lines if any\n",
    "          continue\n",
    "        dataset.append((cols[0].strip(), cols[1].strip()))  # third column is ignored\n",
    "    datasets.append(dataset)\n",
    "  return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bkA5fnHW9yv"
   },
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:30.099380Z",
     "iopub.status.busy": "2025-05-22T01:52:30.099100Z",
     "iopub.status.idle": "2025-05-22T01:52:30.116803Z",
     "shell.execute_reply": "2025-05-22T01:52:30.116301Z",
     "shell.execute_reply.started": "2025-05-22T01:52:30.099355Z"
    },
    "id": "yt9IdSitXcLR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Building vocabulary from training data\n",
    "def buildVocabulary(data):\n",
    "  input_characters = set()   # using set to collect unique characters\n",
    "  target_characters = set()\n",
    "  for input, target in data:\n",
    "    for char in input:\n",
    "      input_characters.add(char)\n",
    "    for char in target:\n",
    "      target_characters.add(char)\n",
    "\n",
    "  input_characters = [' '] + sorted(list(input_characters))       # padding token is ' '\n",
    "  target_characters = [' ', '\\t', '\\n']+ sorted(list(target_characters))   # start token: '\\t'; end token:'\\n'\n",
    "  return input_characters, target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:31.748438Z",
     "iopub.status.busy": "2025-05-22T01:52:31.748191Z",
     "iopub.status.idle": "2025-05-22T01:52:31.752446Z",
     "shell.execute_reply": "2025-05-22T01:52:31.751724Z",
     "shell.execute_reply.started": "2025-05-22T01:52:31.748421Z"
    },
    "id": "lHTNFmiM0xzT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creating dictionaries to get indices for any input, target character\n",
    "def generateTokenIndices(input_characters, target_characters):\n",
    "  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "  return input_token_index, target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:33.351205Z",
     "iopub.status.busy": "2025-05-22T01:52:33.350917Z",
     "iopub.status.idle": "2025-05-22T01:52:33.358432Z",
     "shell.execute_reply": "2025-05-22T01:52:33.357580Z",
     "shell.execute_reply.started": "2025-05-22T01:52:33.351183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generateEmbeddings(data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length):\n",
    "    encoder_input_data = np.zeros((len(data), max_encoder_seq_length), dtype=\"int64\")\n",
    "    decoder_input_data = np.zeros((len(data), max_decoder_seq_length), dtype=\"int64\")\n",
    "    decoder_target_data = np.zeros((len(data), max_decoder_seq_length), dtype=\"int64\")\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(data):\n",
    "        target_text = \"\\t\" + target_text + \"\\n\"  # Adding start and end tokens\n",
    "\n",
    "        # Filling encoder sequence\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t] = input_token_index.get(char, input_token_index[\" \"])\n",
    "        for t in range(len(input_text), max_encoder_seq_length):\n",
    "            encoder_input_data[i, t] = input_token_index[\" \"]\n",
    "\n",
    "        # Filling decoder input and target\n",
    "        for t, char in enumerate(target_text):\n",
    "            decoder_input_data[i, t] = target_token_index.get(char, target_token_index[\" \"])\n",
    "            if t > 0:\n",
    "                decoder_target_data[i, t - 1] = target_token_index.get(char, target_token_index[\" \"])\n",
    "        for t in range(len(target_text), max_decoder_seq_length):\n",
    "            decoder_input_data[i, t] = target_token_index[\" \"]\n",
    "            if t > 0:\n",
    "                decoder_target_data[i, t - 1] = target_token_index[\" \"]\n",
    "\n",
    "    #print(\"train_enc shape:\", encoder_input_data.shape)\n",
    "    #print(\"train_enc dtype:\", encoder_input_data.dtype)\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:50:53.599783Z",
     "iopub.status.busy": "2025-05-21T15:50:53.599124Z",
     "iopub.status.idle": "2025-05-21T15:50:54.930699Z",
     "shell.execute_reply": "2025-05-21T15:50:54.929909Z",
     "shell.execute_reply.started": "2025-05-21T15:50:53.599757Z"
    },
    "id": "2fCqCXk83rQm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.train.tsv\"\n",
    "# val_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.dev.tsv\"\n",
    "# test_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.test.tsv\"\n",
    "# train_data, val_data, test_data = load_data(train_path, val_path, test_path)\n",
    "# input_characters, target_characters = buildVocabulary(train_data)\n",
    "# input_token_index, target_token_index = generateTokenIndices(input_characters, target_characters)\n",
    "# max_encoder_seq_length = max([len(input) for input, _ in train_data])\n",
    "# max_decoder_seq_length = max([len(target) for _, target in train_data]) + 2\n",
    "\n",
    "# train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = generateEmbeddings(train_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n",
    "# val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = generateEmbeddings(val_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n",
    "# test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = generateEmbeddings(test_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:37.173528Z",
     "iopub.status.busy": "2025-05-22T01:52:37.172989Z",
     "iopub.status.idle": "2025-05-22T01:52:37.190669Z",
     "shell.execute_reply": "2025-05-22T01:52:37.189877Z",
     "shell.execute_reply.started": "2025-05-22T01:52:37.173502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, config, input_vocab_size, target_vocab_size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # embedding layers for encoder and decoder\n",
    "        self.embedding_encoder = nn.Embedding(input_vocab_size, config.embedding_dim, padding_idx=0)\n",
    "        self.embedding_decoder = nn.Embedding(target_vocab_size, config.embedding_dim, padding_idx=0)\n",
    "\n",
    "        # considering torch model based on cell type in config\n",
    "        rnn_model_type = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[config.cell_type]\n",
    "\n",
    "        self.encoder = rnn_model_type(\n",
    "            input_size=config.embedding_dim,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.encoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout if config.encoder_layers > 1 else 0,     # adding dropout if number of layers are more than 1\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.decoder = rnn_model_type(\n",
    "            input_size=config.embedding_dim,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.decoder_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout if config.decoder_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        # projecting encoder hidden state to match decoder shape\n",
    "        self.h_proj = nn.Linear(config.hidden_size * config.encoder_layers,\n",
    "                                config.hidden_size * config.decoder_layers)\n",
    "\n",
    "        if config.cell_type == 'LSTM':\n",
    "            self.c_proj = nn.Linear(config.hidden_size * config.encoder_layers,\n",
    "                                    config.hidden_size * config.decoder_layers)\n",
    "        else:\n",
    "            self.c_proj = None\n",
    "\n",
    "        # output layer\n",
    "        self.fc_out = nn.Linear(config.hidden_size, target_vocab_size)\n",
    "\n",
    "    def _transform_hidden(self, h_enc):\n",
    "        # h_enc shape: (layers, batch, hidden_size)\n",
    "        # reshaping encoder hidden state for decoder\n",
    "        batch_size = h_enc.size(1)\n",
    "        h_flat = h_enc.permute(1, 0, 2).contiguous().view(batch_size, -1)  # (batch, layers*hidden_size)\n",
    "        h_proj = self.h_proj(h_flat)  # (batch, dec_layers*hidden_size)\n",
    "        h_proj = h_proj.view(batch_size, self.config.decoder_layers, self.config.hidden_size).permute(1, 0, 2).contiguous()\n",
    "        return h_proj\n",
    "\n",
    "    def _transform_cell(self, c_enc):\n",
    "        if c_enc is None:\n",
    "            return None\n",
    "        batch_size = c_enc.size(1)\n",
    "        c_flat = c_enc.permute(1, 0, 2).contiguous().view(batch_size, -1)\n",
    "        c_proj = self.c_proj(c_flat)\n",
    "        c_proj = c_proj.view(batch_size, self.config.decoder_layers, self.config.hidden_size).permute(1, 0, 2).contiguous()\n",
    "        return c_proj\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_embedded = self.embedding_encoder(encoder_input)\n",
    "        decoder_embedded = self.embedding_decoder(decoder_input)\n",
    "\n",
    "        # print shapes for debugging\n",
    "        #print(encoder_embedded.shape)\n",
    "        #print(decoder_embedded.shape)\n",
    "\n",
    "        if self.config.cell_type == 'LSTM':\n",
    "            encoder_outputs, (h, c) = self.encoder(encoder_embedded)\n",
    "            h_dec = self._transform_hidden(h)\n",
    "            c_dec = self._transform_cell(c)\n",
    "            decoder_outputs, _ = self.decoder(decoder_embedded, (h_dec, c_dec))\n",
    "        else:\n",
    "            encoder_outputs, h = self.encoder(encoder_embedded)\n",
    "            h_dec = self._transform_hidden(h)\n",
    "            decoder_outputs, _ = self.decoder(decoder_embedded, h_dec)\n",
    "\n",
    "        output = self.fc_out(decoder_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "def train_model(model, train_data, val_data, config, target_vocab_size, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)   # ignoring padding values in loss (padded with zero here)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    train_enc, train_dec, train_tgt = train_data\n",
    "    val_enc, val_dec, val_tgt = val_data\n",
    "\n",
    "    train_enc = torch.tensor(train_enc, dtype=torch.long)\n",
    "    train_dec = torch.tensor(train_dec, dtype=torch.long)\n",
    "    train_tgt = torch.tensor(train_tgt, dtype=torch.long)\n",
    "\n",
    "    #print(f\"After torch tensor conversion shape: f{train_enc.shape} , f{train_dec.shape}, f{train_tgt.shape}\")\n",
    "\n",
    "    val_enc = torch.tensor(val_enc, dtype=torch.long)\n",
    "    val_dec = torch.tensor(val_dec, dtype=torch.long)\n",
    "    val_tgt = torch.tensor(val_tgt, dtype=torch.long)\n",
    "\n",
    "    # batching training data\n",
    "    dataset = TensorDataset(train_enc, train_dec, train_tgt)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    start_token_idx = 1  # '\\t' in target_token_index\n",
    "    end_token_idx = 2    # '\\n' in target_token_index\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for encoder_batch, decoder_input_batch, decoder_target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(encoder_batch.to(device), decoder_input_batch.to(device))\n",
    "            loss = criterion(output.view(-1, target_vocab_size), decoder_target_batch.to(device).view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        wandb.log({\n",
    "            \"epoch\" : epoch+1,\n",
    "            \"train_loss\": total_loss,\n",
    "            \n",
    "        })\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_tokens = 0\n",
    "            total_tokens = 0\n",
    "            val_loss = 0\n",
    "\n",
    "            val_dataset = TensorDataset(val_enc, val_dec, val_tgt)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "            for val_enc_batch, val_dec_batch, val_tgt_batch in val_loader:\n",
    "                val_enc_batch = val_enc_batch.to(device)\n",
    "                val_dec_batch = val_dec_batch.to(device)\n",
    "                val_tgt_batch = val_tgt_batch.to(device) \n",
    "\n",
    "                output = model(val_enc_batch, val_dec_batch)\n",
    "                loss = criterion(output.view(-1, target_vocab_size), val_tgt_batch.view(-1))\n",
    "                val_loss += loss.item()                \n",
    "\n",
    "                predictions = output.argmax(dim=-1)\n",
    "                mask = val_tgt_batch != 0  # ignore padding\n",
    "                correct_tokens += (predictions == val_tgt_batch).masked_select(mask).sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "\n",
    "            val_accuracy = correct_tokens / total_tokens\n",
    "\n",
    "            wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "        #print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:40.124014Z",
     "iopub.status.busy": "2025-05-22T01:52:40.123386Z",
     "iopub.status.idle": "2025-05-22T01:52:40.129779Z",
     "shell.execute_reply": "2025-05-22T01:52:40.128992Z",
     "shell.execute_reply.started": "2025-05-22T01:52:40.123983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==================== SWEEP CONFIG ====================\n",
    "sweep_config = {\n",
    "    'name': 'sweep_final1',\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embedding_dim': {'values': [16, 32, 64]},\n",
    "        'hidden_size': {'values': [32, 64, 128]},\n",
    "        'encoder_layers': {'values': [1, 2]},\n",
    "        'decoder_layers': {'values': [1, 2]},\n",
    "        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n",
    "        'dropout': {'values': [0.2, 0.3]},\n",
    "        'lr': {'values': [0.001]},\n",
    "        'epochs': {'value': 10},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace train, test, validation dataset paths in below cell as marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:40.758793Z",
     "iopub.status.busy": "2025-05-22T01:52:40.758291Z",
     "iopub.status.idle": "2025-05-22T01:52:40.764473Z",
     "shell.execute_reply": "2025-05-22T01:52:40.763615Z",
     "shell.execute_reply.started": "2025-05-22T01:52:40.758770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sweep_train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    wandb.run.name = f\"{config.cell_type}_enc_{config.encoder_layers}_dec_{config.decoder_layers}_{config.epochs}_emb_{config.embedding_dim}_hs_{config.hidden_size}\"\n",
    "\n",
    "    # Replace these paths with your train, val, test dataset paths\n",
    "    train_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.train.tsv\"\n",
    "    val_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.dev.tsv\"\n",
    "    test_path = \"/kaggle/input/assignment3-telugu-dakshinadataset/te.translit.sampled.test.tsv\"\n",
    "\n",
    "    # load input-output string pairs from each split\n",
    "    train_data, val_data, test_data = load_data(train_path, val_path, test_path)\n",
    "\n",
    "    # building vocabularies from training data\n",
    "    input_characters, target_characters = buildVocabulary(train_data)\n",
    "\n",
    "    # creating character-to-index mappings\n",
    "    input_token_index, target_token_index = generateTokenIndices(input_characters, target_characters)\n",
    "\n",
    "    max_encoder_seq_length = max([len(input) for input, _ in train_data])\n",
    "    max_decoder_seq_length = max([len(target) for _, target in train_data]) + 2\n",
    "    \n",
    "    train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = generateEmbeddings(train_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n",
    "    val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = generateEmbeddings(val_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n",
    "    test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = generateEmbeddings(test_data, input_token_index, target_token_index, max_encoder_seq_length, max_decoder_seq_length)\n",
    "\n",
    "\n",
    "    model = Seq2SeqModel(config, len(input_token_index), len(target_token_index))\n",
    "\n",
    "    # train the model and log metrics to wandb\n",
    "    train_model(model, (train_encoder_input_data, train_decoder_input_data, train_decoder_target_data), (val_encoder_input_data, val_decoder_input_data, val_decoder_target_data), config, len(target_token_index), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace wandb key and project, entity details with your information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T01:52:43.425031Z",
     "iopub.status.busy": "2025-05-22T01:52:43.424752Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikhithaa\u001b[0m (\u001b[33mnikhithaa-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: i6pogcjt\n",
      "Sweep URL: https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yb5efjel with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015256-yb5efjel</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel' target=\"_blank\">neat-sweep-1</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>1231.54235</td></tr><tr><td>val_accuracy</td><td>0.81161</td></tr><tr><td>val_loss</td><td>102.90616</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-sweep-1</strong> at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/yb5efjel</a><br> View project at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250522_015256-yb5efjel/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4mkqoq24 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015443-4mkqoq24</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24' target=\"_blank\">genial-sweep-2</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>563.46399</td></tr><tr><td>val_accuracy</td><td>0.8779</td></tr><tr><td>val_loss</td><td>62.30132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-sweep-2</strong> at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/4mkqoq24</a><br> View project at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250522_015443-4mkqoq24/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9wxlw2ka with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015619-9wxlw2ka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka' target=\"_blank\">lively-sweep-3</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇█▇▇█▇</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>3503.59712</td></tr><tr><td>val_accuracy</td><td>0.38551</td></tr><tr><td>val_loss</td><td>348.67116</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-3</strong> at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/9wxlw2ka</a><br> View project at: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250522_015619-9wxlw2ka/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3i1wsj9a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250522_015740-3i1wsj9a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/3i1wsj9a' target=\"_blank\">devoted-sweep-4</a></strong> to <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/sweeps/i6pogcjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/3i1wsj9a' target=\"_blank\">https://wandb.ai/nikhithaa-iit-madras/DA6401_Assignment3/runs/3i1wsj9a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=\"XXXX\")\n",
    "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment3',entity=\"nikhithaa-iit-madras\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=50)   # remove count if you dont want to limit number of runs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7464076,
     "sourceId": 11876572,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7464083,
     "sourceId": 11876589,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
